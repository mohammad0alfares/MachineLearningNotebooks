{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "DIlzJbCpmo0R"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammad0alfares/MachineLearningNotebooks/blob/master/Logistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzmEcNHzuBI3",
        "colab_type": "text"
      },
      "source": [
        "# Clone the Source GitHub Reporsitory \n",
        "We need to clone some source files to be used throughtout this tutorial from the GitHub reprository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmP4GrRNudXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./MachineLearning\n",
        "!git clone https://github.com/mkjubran/MachineLearning.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIlzJbCpmo0R",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression\n",
        "**Introduction**\n",
        "\n",
        "In this section, we will use **logistic regression** to solve few classification problems. We will solve binary and multiclass problems using **logistic regression**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrMDfwQQzBEn",
        "colab_type": "text"
      },
      "source": [
        "**Readings and Resources**\n",
        "\n",
        "[1] https://medium.com/greyatom/logistic-regression-89e496433063"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOQvdG9QW1Yv",
        "colab_type": "text"
      },
      "source": [
        "# Case #1: Studying Hours and Passing Exams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7ietEiOzhNl",
        "colab_type": "text"
      },
      "source": [
        "In this section we will use logistic regression to infer whether a student will pass or fail an exam based on the number of hours the student spends preparing for the exam. A dataset for few students that includes the number of study hours and whether they pass (1) or fail (0) the exam. This is a binary classification problem that can be solved using logistic regression as will be shown next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RSwASngm9_9",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Read the input data (number of study hours and exam pass or fail) from the csv file (HoursPassExam.csv) file. Use the pandas library (https://pandas.pydata.org/) to read the data from the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQX2iq_fnJOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./MachineLearning/2_logistic/HoursPassExam.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJA9kY1o6_lr",
        "colab_type": "text"
      },
      "source": [
        "To view the dataset, we will use the scatter plot from matplotlib library as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryh3BJOV7eo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(df['hours'],df['pass'],color = 'red', marker = '+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39M2I3LU70dB",
        "colab_type": "text"
      },
      "source": [
        "As can be seen, the output (y) is binary; 0 for failing the exam and 1 for passing the exam. Also, the chances of passing the exam increases when the number of studying for the exam increases. Let us divide the dataset into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uIoyL855AoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x = df[['hours']]\n",
        "y = df['pass']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "print('size of test dataset = {}, size of traing data = {}, percentage = {}%'.format(len(x_test),len(x_train),len(x_test)*100/(len(x_test) + len(x_train))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUTs3Ywp5ARf",
        "colab_type": "text"
      },
      "source": [
        "Let us now try to use linear regresison to fit this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtqBhjVN8Hgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x_train,y_train)\n",
        "print('coef=',reg.coef_)\n",
        "print('intercept=',reg.intercept_)\n",
        "print('score=',reg.score(x_train,y_train))\n",
        "plt.scatter(x_train,y_train,color = 'red', marker = '+', label = 'Data')\n",
        "plt.plot(x_train,reg.predict(x_train) , label = 'Linear')\n",
        "plt.legend()\n",
        "plt.xlabel('Hourse')\n",
        "plt.ylabel('Pass Exam (0:pass,1:fail)')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HHv5Gko8_20",
        "colab_type": "text"
      },
      "source": [
        "As can be observed from the figure above, the linear regresison fails to fit the data (also training accuracy ~= 56%). Let us try the logitic regression to fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBkR7WTW9T7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "logreg = linear_model.LogisticRegression()\n",
        "logreg.fit(x_train,y_train)\n",
        "print(logreg.coef_)\n",
        "print(logreg.intercept_)\n",
        "print(logreg.score(x_train,y_train))\n",
        "print(logreg.score(x_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZplRx6NhjbWq",
        "colab_type": "text"
      },
      "source": [
        "The training and testing accuracy of the logistic regression are higher than linear regression. This means that the logitic regresison fit the data better. Let us plot the logitic regression curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ9X3woTjbhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(x,y,color = 'red', marker = '+', label = 'Data')\n",
        "plt.plot(x,reg.predict(x) , label = 'Linear')\n",
        "plt.xlabel('Hourse')\n",
        "plt.ylabel('Pass Exam (0:pass,1:fail)')\n",
        "\n",
        "import numpy as np\n",
        "x_sigmoid = np.sort(np.array(x),axis=0)\n",
        "##plot the logistic regression\n",
        "plt.plot(x_sigmoid,logreg.predict(pd.DataFrame(x_sigmoid)), label = 'Logitic', color ='orange')\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q33xV2b7Ab1",
        "colab_type": "text"
      },
      "source": [
        "We observe from the above figure that the logistic regression curve (orange line) fits better the dataset as compared to the linear regression curve (blue line). Let us also plot the sigmoid curve on the same figure using the logistic coefficient and inception. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXGkgz1S7Ak8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(x,y,color = 'red', marker = '+', label = 'Data')\n",
        "plt.plot(x,reg.predict(x) , label = 'Linear')\n",
        "plt.xlabel('Hourse')\n",
        "plt.ylabel('Pass Exam (0:pass,1:fail)')\n",
        "\n",
        "import numpy as np\n",
        "x_sigmoid = np.sort(np.array(x),axis=0)\n",
        "##plot the logistic regression\n",
        "plt.plot(x_sigmoid,logreg.predict(pd.DataFrame(x_sigmoid)), label = 'Logitic', color ='orange')\n",
        "\n",
        "import math\n",
        "## plot the sigmoid function\n",
        "a=float(logreg.coef_)\n",
        "b=float(logreg.intercept_)\n",
        "y_sigmoid = [1 / ( 1 + math.exp(-1 * ( a * val + b ))) for val in x_sigmoid]\n",
        "plt.plot(x_sigmoid,y_sigmoid, label = 'Sigmoid', color ='green')\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAgRJfnHLayp",
        "colab_type": "text"
      },
      "source": [
        "We could also add the probability of classification for each point in the datset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7baRcAetJYV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(x,y,color = 'red', marker = '+', label = 'Data')\n",
        "plt.plot(x,reg.predict(x) , label = 'Linear')\n",
        "plt.xlabel('Hourse')\n",
        "plt.ylabel('Pass Exam (0:pass,1:fail)')\n",
        "\n",
        "import numpy as np\n",
        "x_sigmoid = np.sort(np.array(x),axis=0)\n",
        "##plot the logistic regression\n",
        "plt.plot(x_sigmoid,logreg.predict(pd.DataFrame(x_sigmoid)), label = 'Logitic', color ='orange')\n",
        "\n",
        "import math\n",
        "## plot the sigmoid function\n",
        "a=float(logreg.coef_)\n",
        "b=float(logreg.intercept_)\n",
        "y_sigmoid = [1 / ( 1 + math.exp(-1 * ( a * val + b ))) for val in x_sigmoid]\n",
        "plt.plot(x_sigmoid,y_sigmoid, label = 'Sigmoid', color ='green')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "## geting probaility of classification for each point in the datset\n",
        "y_prob = logreg.predict_proba(x)\n",
        "plt.scatter(x,y_prob[:,1],color = 'black', marker = '+', label = 'prob')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg0bQjS5CMlB",
        "colab_type": "text"
      },
      "source": [
        "We observe the following from the above figure: \\\\\n",
        "1- Sigmoid fits the dataset better than linear regression. It's values are between 0 and 1. \\\\\n",
        "2- Linear regression extends to values less than zero and more than one which doesn't match the dataset (pass=1, fail=0). \\\\\n",
        "3- The logistic curve is zero when the sigmoid is less than 0.5, and the logistic curve is one when the sigmoid is more than 0.5 (this can be adjusted depending on the classification problem). \\\\\n",
        "4- The sigmoid curve is the probability of classification. \\\\ \n",
        "\n",
        "Also for logistic regression, the intercept (b) moves the curve left and right and the slope (a) defines the steepness of the curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH1COvVuXwR5",
        "colab_type": "text"
      },
      "source": [
        "# Case #2: HR Analysis\n",
        "\n",
        "In this section, we will analyze the data of employees of a company. This data includes some information about the employees who are working at the company and those who left the company. Our objective is to predict whether an existing employee would leave the company based on his/her current status. This will help us decide to offer the employee some incentives to keep him/her in the company. This could also be used to plan early to hire new employees.$^{[1]}$\n",
        "\n",
        "[1] https://codebasicshub.com/tutorial/machine-learning/logistic-regression-binary-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eR70Y6nbQYs",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Read the input data from the csv file (HR_comma_sep.csv) file. Dataset is downloaded from Kaggle. Link: https://www.kaggle.com/giripujar/hr-analytics. Use the pandas library (https://pandas.pydata.org/) to read the data from the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZrjhWA3YRcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "HR = pd.read_csv('./MachineLearning/2_logistic/HR_comma_sep.csv')\n",
        "HR.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEVwPozTbhm6",
        "colab_type": "text"
      },
      "source": [
        "Before applying regression to the data, we will explore and analyze the data to determine the features that influence the decision of the employees to remain or leave the company."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PfSFCycBBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "left = HR[HR.left==1] ## employees who left the company \n",
        "No_left= left.shape[0]\n",
        "remain = HR[HR.left==0] ## employees who remain at the company \n",
        "No_remain = remain.shape[0]\n",
        "Per_left = No_left / (No_left + No_remain)\n",
        "\n",
        "print('No_left = {}, No_remain = {} , Percentage of left = {} %'.format(No_left,No_remain,Per_left*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R61IR-gJdNQQ",
        "colab_type": "text"
      },
      "source": [
        "About $23\\%$ employees left the company. Now, let us check which features are mostly affecting the decision of employees to leave or remain in the company. To do this, we will measure the average of each numeric feature for employees to remain or leave the company.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmI7sVO4d6vG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HR.groupby('left').mean() #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY7uKIrGeOIQ",
        "colab_type": "text"
      },
      "source": [
        "We may conclude the following from the table above: \\\\\n",
        "1- Employees who remain in the company has higher satisfaction_level and thus it is a good indicator for our regression/classifier (good feature) \\\\\n",
        "2- The last_evaluation, number of projects, and time_spend_company scores are almost independent of the employees remain or leave the company \\\\\n",
        "3- The average_montly_hours for employees who left the company are higher than those who remained which could be an indicator (good feature) \\\\\n",
        "4- The promotion_last_5years feature of employees remaining in the company is much higher than those left the company (good feature) \\\\\n",
        "5- Work_accident is also an indicator so it is a good feature.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MlXIibs9Z8R",
        "colab_type": "text"
      },
      "source": [
        "Let us also check the quality of the categories' features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Rf5SjP6Ix8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(HR.salary,HR.left)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWOZE5NBy5ru",
        "colab_type": "text"
      },
      "source": [
        "The salary table shows that emloyees with high salaries are more likely to stay in the company. So it is a good feature. To visualize this we make a bar plot as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xIPIbJ5965b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(HR.salary,HR.left).plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q0QfTZjzPs2",
        "colab_type": "text"
      },
      "source": [
        "We need also to investigate the department feature as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCHfkCyz-B-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(HR.Department,HR.left).plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT1gQWr30YT0",
        "colab_type": "text"
      },
      "source": [
        "The department type has a minor effect on the decision of employees to stay or leave the company. It doesn't look a major factor and thus we will ignore this feature. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3okSyzEL8HzB",
        "colab_type": "text"
      },
      "source": [
        "Based on the above analysis, we will create the following table which includes only the good (important, major) features affecting employees' decisions to stay or leave the company"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2I6qW3L5XS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HR_GF = HR[['left','satisfaction_level','average_montly_hours','Work_accident','promotion_last_5years', 'salary']]\n",
        "HR_GF.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ho-hLuj1bTz",
        "colab_type": "text"
      },
      "source": [
        "To prepare the data for classification (using logistic regression), we will apply one hot encoding for the categorical features (salary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnY3Rx-V11Dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dm = pd.get_dummies(HR_GF.salary)\n",
        "HR_GF_merged = pd.concat([HR_GF,dm],axis=1)\n",
        "HR_GF_merged = HR_GF_merged.drop(['salary','medium'],axis=1)\n",
        "HR_GF_merged.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYVPPi0N4AR_",
        "colab_type": "text"
      },
      "source": [
        "Let us define input (x) and output (y) of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ercG4Iwd4A-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = HR_GF_merged.drop('left',axis=1)\n",
        "y = HR_GF_merged.left"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfEJ_b-i3Vee",
        "colab_type": "text"
      },
      "source": [
        "Before classification, we need to split the datset into test and training parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTFsz9D_3oF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "print('size of test dataset = {}, size of traing data = {}, percentage = {}%'.format(len(x_test),len(x_train),len(x_test)*100/(len(x_test) + len(x_train))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRyJhrij4oAB",
        "colab_type": "text"
      },
      "source": [
        "Now, we are ready to apply logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wVAxuJm4pMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "logreg = linear_model.LogisticRegression()\n",
        "logreg.fit(x_train,y_train)\n",
        "print(logreg.coef_)\n",
        "print(logreg.intercept_)\n",
        "print(logreg.score(x_train,y_train))\n",
        "print(logreg.score(x_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMLQxxK36HRx",
        "colab_type": "text"
      },
      "source": [
        "**Comment on the traning and testing accuracies?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIgCwet8SjNR",
        "colab_type": "text"
      },
      "source": [
        "# Case #3: Recognition of Handwitten Digits\n",
        "\n",
        "In this section, we will try to recognize handwritten digits using logistic regression. We will be using a standard dataset available through the sklearn library called \"load_digits\".$^{[1][2]}$\n",
        "\n",
        "[1] https://scikit-learn.org/stable/tutorial/basic/tutorial.html#introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrH_szfO9r8F",
        "colab_type": "text"
      },
      "source": [
        "**Readings and Resources**\n",
        "\n",
        "1- https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py\n",
        "\n",
        "2- https://scikit-learn.org/stable/tutorial/basic/tutorial.html#introduction\n",
        "\n",
        "3- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQYOLvyo966W",
        "colab_type": "text"
      },
      "source": [
        "In the beginning, we will load the dataset as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thCgqt3KSpwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGRXkAwRMDWg",
        "colab_type": "text"
      },
      "source": [
        "A dataset is a dictionary-like object that holds all the data and some metadata about the data. Let us explore the content of the digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qlgSoRTAhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir(digits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEbzkkNqMLZf",
        "colab_type": "text"
      },
      "source": [
        "The digits.data contains the features that will be used to classify the digits samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lw41j9WUlr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(digits.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_ZbG54iNTgV",
        "colab_type": "text"
      },
      "source": [
        "The digits.images contains the images of the digits samples. They can be viewed using the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsM4ZMMWXGBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.gray()\n",
        "plt.matshow(digits.images[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK1cRXeTNmwf",
        "colab_type": "text"
      },
      "source": [
        "The ground truth of the datset is stored in the digits.taget"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX5YG-S-UxiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(digits.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc0cKNwJN45-",
        "colab_type": "text"
      },
      "source": [
        "After exploring the content of the digits dataset, we will design a classified using logistic regression. First, we decide the input feature vector (x) and the ground truth (y) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jSk9q-BU-1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = digits.data\n",
        "y = digits.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oti7Y9GiOg8M",
        "colab_type": "text"
      },
      "source": [
        "Then we split the datset into testing and training parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBr_laJGVLRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "print('size of test dataset = {}, size of traing data = {}, percentage = {}%'.format(len(x_test),len(x_train),len(x_test)*100/(len(x_test) + len(x_train))))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9KDqCdXOn3M",
        "colab_type": "text"
      },
      "source": [
        "Here we will train the logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7QAzJ0pV6Am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "logreg = linear_model.LogisticRegression()\n",
        "logreg.fit(x_train,y_train)\n",
        "print(logreg.score(x_train,y_train))\n",
        "print(logreg.score(x_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig8JsexROuls",
        "colab_type": "text"
      },
      "source": [
        "**Comment on the training and testing accuracies.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsPsIfUfMK83",
        "colab_type": "text"
      },
      "source": [
        "To predict the types of test samples and store it is y_pred run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uyTP8bKWvFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = logreg.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnRMgtUVO-a1",
        "colab_type": "text"
      },
      "source": [
        "Sometimes, we wish to know where did the model fail. This can be achieved using what is called the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bldP8mHmW3U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jcSo94ZPZ71",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix is a square-matrix that shows the relationship between the ground truth and the predicted values. For example, the number in the left-top cell indicates the number of zero digits in the dataset that were predicted as zeros. In the last row which represents the prediction of the 9 digit, the first cell in the row includes the number of 0 digits that are classified as 9, the second cell containing the number of 1 digits that are predicted as 9 and so on. \n",
        "\n",
        "We can also use the  seaborn library to view the confusion matrix as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL6IeTlvYgWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sn\n",
        "sn.heatmap(cm,annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK2x4REQUguc",
        "colab_type": "text"
      },
      "source": [
        "We could also increase the size of the figure using matplot.pyplot properties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjvAC3-ZNPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(cm,annot=True)\n",
        "plt.xlabel('ground truth')\n",
        "plt.ylabel('predicted')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4rcW3Gk8_1m",
        "colab_type": "text"
      },
      "source": [
        "Let us try to find out how did the logistic classified a specific digit. In the next code, we will visualize the images, predicted and target values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmX8TWGF9J4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "fig = plt.figure(figsize=(12, 24))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "digit_visualize = 5 # the digit that we want to visualize\n",
        "\n",
        "cnt = 0\n",
        "i = 0\n",
        "while (i < 128) and (i < len(y_test)) :\n",
        "    if y_test[i] == digit_visualize:\n",
        "       Idx = np.where(np.prod(digits.data == x_test[i,:],axis = -1))\n",
        "       ax = fig.add_subplot(16, 8, cnt + 1, xticks=[], yticks=[])\n",
        "       ax.imshow(digits.images[int(Idx[0])], cmap=plt.cm.binary, interpolation='nearest')\n",
        "       # label the image with the target value\n",
        "       ax.text(0, 7, str(y_test[i]))\n",
        "       ax.text(6.5, 7, str(y_pred[i]))\n",
        "       cnt+=1\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpeTBau-9Ush",
        "colab_type": "text"
      },
      "source": [
        "Let us try to find out in more details where did the logistic failed. In the next code, we will visualize the images, predicted and target values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfOjaEoI9eYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(12, 24))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "cnt = 0\n",
        "i = 0\n",
        "while (i < 128) and (i < len(y_test)) :\n",
        "    if y_test[i] != y_pred[i]:\n",
        "       Idx = np.where(np.prod(digits.data == x_test[i,:],axis = -1))\n",
        "       ax = fig.add_subplot(16, 8, cnt + 1, xticks=[], yticks=[])\n",
        "       ax.imshow(digits.images[int(Idx[0])], cmap=plt.cm.binary, interpolation='nearest')\n",
        "       # label the image with the target value\n",
        "       ax.text(0, 7, str(y_test[i]))\n",
        "       ax.text(6.5, 7, str(y_pred[i]))\n",
        "       cnt+=1\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clLdKNnsVLSK",
        "colab_type": "text"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "**1) Iris identification**: use logistic regression to build a classification model for the iris dataset stored in the sklearn library. You may import the dataset using \"from sklearn.datasets import load_iris\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fIjAB49qqkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./MachineLearning/2_logistic/HoursPassExam.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}