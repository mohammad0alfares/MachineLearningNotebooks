{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DecisionTree.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammad0alfares/MachineLearningNotebooks/blob/master/DecisionTree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzmEcNHzuBI3",
        "colab_type": "text"
      },
      "source": [
        "# Clone the Source GitHub Reporsitory \n",
        "We need to clone some source files to be used throughtout this tutorial from a GitHub reprository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmP4GrRNudXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./MachineLearning\n",
        "!git clone https://github.com/mkjubran/MachineLearning.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIlzJbCpmo0R",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees\n",
        "**Introduction**\n",
        "\n",
        "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.$^{[1]}$\n",
        "\n",
        "[1] https://scikit-learn.org/0.15/modules/tree.html#tree\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrMDfwQQzBEn",
        "colab_type": "text"
      },
      "source": [
        "**Readings and Resources**\n",
        "\n",
        "[1] https://scikit-learn.org/0.15/modules/tree.html#tree\n",
        "\n",
        "[2] https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\n",
        "\n",
        "[3] https://towardsdatascience.com/boosting-the-accuracy-of-your-machine-learning-models-f878d6a2d185"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOQvdG9QW1Yv",
        "colab_type": "text"
      },
      "source": [
        "# Case #1: Studying Hours and Passing Exams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7ietEiOzhNl",
        "colab_type": "text"
      },
      "source": [
        "In this section we will use **decision tree** to infer whether a student will pass or fail an exam based on the number of hours the student spends preparing for the exam. A dataset for few students that includes the number of study hours and whether they pass (1) or fail (0) the exam. This is a binary classification problem that can be solved using **decision tree** as will be shown next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RSwASngm9_9",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Read the input data (number of study hours and exam pass or fail) from the csv file (HoursPassExam.csv) file. Use the pandas library (https://pandas.pydata.org/) to read the data from the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQX2iq_fnJOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./MachineLearning/3_decision_tree/HoursPassExam.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9j_pcYIQiYa",
        "colab_type": "text"
      },
      "source": [
        "To get some information about the read dataset including parameters and type of fields and features use the pandas info method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6XIJhyRQe5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJA9kY1o6_lr",
        "colab_type": "text"
      },
      "source": [
        "To view the dataset, we will use the scatter plot from matplotlib library as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryh3BJOV7eo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(df['hours'],df['pass'],color = 'red', marker = '+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39M2I3LU70dB",
        "colab_type": "text"
      },
      "source": [
        "As can be seen, the output (y) is binary; 0 for failing the exam and 1 for passing the exam. Also, the chances of passing the exam increases when the number of studying hours for the exam increases. Let us divide the dataset into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uIoyL855AoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x = df[['hours']]\n",
        "y = df['pass']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "print('size of test dataset = {}, size of traing data = {}, percentage = {}%'.format(len(x_test),len(x_train),len(x_test)*100/(len(x_test) + len(x_train))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6HodV5tPjPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (type(x))\n",
        "print (type(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3B2xf9y60Xm",
        "colab_type": "text"
      },
      "source": [
        "Next we will train the **decision tree** model and compute its accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBkR7WTW9T7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "model_dt = tree.DecisionTreeClassifier()\n",
        "model_dt.fit(x_train,y_train)\n",
        "ACC_train_dt = model_dt.score(x_train,y_train)\n",
        "ACC_test_dt = model_dt.score(x_test,y_test)\n",
        "print(ACC_train_dt)\n",
        "print(ACC_train_dt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmCA870Go7Q1",
        "colab_type": "text"
      },
      "source": [
        "Let us try to compare **DT** with logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb80tgIqo7By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(x_train, y_train)\n",
        "ACC_train_lr = model_lr.score(x_train, y_train)\n",
        "ACC_test_lr = model_lr.score(x_test, y_test)\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['Accuracy', 'Logistic (%)' , 'DT (%)'])\n",
        "t.add_row(['Training', ACC_train_lr*100, ACC_train_dt*100])\n",
        "t.add_row(['Testing', ACC_test_lr*100, ACC_test_dt*100])\n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZplRx6NhjbWq",
        "colab_type": "text"
      },
      "source": [
        "**Comment of the training and testing accuracies of DT as compared to logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH1COvVuXwR5",
        "colab_type": "text"
      },
      "source": [
        "# Case #2: HR Analysis\n",
        "\n",
        "In this section, we will analyze the data of employees of a company. This data includes some information about the employees who are working at the company and those who left the company. Our objective is to predict whether an existing employee would leave the company based on his/her current status. This will help us decide to offer the employee some incentives to keep him/her in the company. This could also be used to plan early to hire new employees. We will try to solve this problem using **decision trees**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eR70Y6nbQYs",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Read the input data from the csv file (HR_comma_sep.csv) file. Dataset is downloaded from Kaggle. Link: https://www.kaggle.com/giripujar/hr-analytics. Use the pandas library (https://pandas.pydata.org/) to read the data from the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZrjhWA3YRcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "HR = pd.read_csv('./MachineLearning/3_decision_tree/HR_comma_sep.csv')\n",
        "HR.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RpPISukQ7AW",
        "colab_type": "text"
      },
      "source": [
        "To get some information about the read dataset use the pandas info method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trVn1MFTQ7gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HR.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEVwPozTbhm6",
        "colab_type": "text"
      },
      "source": [
        "Before applying regression to the data, we will explore and analyze the data to determine the features that influence the decision of the students to remain or leave the company."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PfSFCycBBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "left = HR[HR.left==1] ## employees who left the company \n",
        "No_left= left.shape[0]\n",
        "remain = HR[HR.left==0] ## employees who remain at the company \n",
        "No_remain = remain.shape[0]\n",
        "Per_left = No_left / (No_left + No_remain)\n",
        "\n",
        "print('No_left = {}, No_remain = {} , Percentage of left = {} %'.format(No_left,No_remain,Per_left*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R61IR-gJdNQQ",
        "colab_type": "text"
      },
      "source": [
        "About $23\\%$ employees left the company. Now, let us check which features are mostly affecting the decision of employees to leave or remain in the company. To do this, we will measure the average of each numeric feature for employees to remain or leave the company.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmI7sVO4d6vG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HR.groupby('left').mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY7uKIrGeOIQ",
        "colab_type": "text"
      },
      "source": [
        "We may conclude the following from the table above: \\\\\n",
        "1- Employees who remain in the company has higher satisfaction_level and thus it is a good indicator for our regression/classifier (good feature) \\\\\n",
        "2- The last_evaluation, number of projects, and time_spend_company scores are almost independent of the employees remain or leave the company \\\\\n",
        "3- The average_montly_hours for employees who left the company are higher than those who remained which could be an indicator (good feature) \\\\\n",
        "4- The promotion_last_5years feature of employees remaining in the company is much higher than those left the company (good feature) \\\\\n",
        "5- Work_accident is also an indicator so it is a good feature.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MlXIibs9Z8R",
        "colab_type": "text"
      },
      "source": [
        "Let us also check the quality of the categories' features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Rf5SjP6Ix8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(HR.salary,HR.left)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWOZE5NBy5ru",
        "colab_type": "text"
      },
      "source": [
        "The salary table shows that emloyees with high salaries are more likely to stay in the company. So it is a good feature. To visualize this we make a bar plot as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xIPIbJ5965b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(HR.salary,HR.left).plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q0QfTZjzPs2",
        "colab_type": "text"
      },
      "source": [
        "We need also to investigate the department feature as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCHfkCyz-B-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(HR.Department,HR.left).plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT1gQWr30YT0",
        "colab_type": "text"
      },
      "source": [
        "The department type has a minor effect on the decision of employees to stay or leave the company. It doesn't look a major factor and thus we will ignore this feature. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3okSyzEL8HzB",
        "colab_type": "text"
      },
      "source": [
        "Based on the above analysis, we will create the following table which includes only the good (important, major) features affecting employees decisions to stay or leave the company"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2I6qW3L5XS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HR_GF = HR[['left','satisfaction_level','average_montly_hours','Work_accident','promotion_last_5years', 'salary']]\n",
        "HR_GF.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzvfmDawazOO",
        "colab_type": "text"
      },
      "source": [
        "Let us plot this data for better visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6yvb358az4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "HR_GF_0 = HR_GF[HR_GF['left'] == 0]\n",
        "HR_GF_1 = HR_GF[HR_GF['left'] == 1]\n",
        "\n",
        "fig, axes = plt.subplots(2, 4,figsize = (20,10))\n",
        "axes[0,0].scatter(HR_GF_0['satisfaction_level'], HR_GF_0['average_montly_hours'], color = 'blue', marker ='+')\n",
        "axes[0,0].set_xlabel('satisfaction_level')\n",
        "axes[0,0].set_ylabel('average_montly_hours')\n",
        "\n",
        "axes[0,1].scatter(HR_GF_0['satisfaction_level'], HR_GF_0['Work_accident'], color = 'blue', marker ='+')\n",
        "axes[0,1].set_xlabel('satisfaction_level')\n",
        "axes[0,1].set_ylabel('Work_accident')\n",
        "\n",
        "axes[0,2].scatter(HR_GF_0['satisfaction_level'], HR_GF_0['promotion_last_5years'], color = 'blue', marker ='+')\n",
        "axes[0,2].set_xlabel('satisfaction_level')\n",
        "axes[0,2].set_ylabel('promotion_last_5years')\n",
        "\n",
        "axes[0,3].scatter(HR_GF_0['satisfaction_level'], HR_GF_0['salary'], color = 'blue', marker ='+')\n",
        "axes[0,3].set_xlabel('satisfaction_level')\n",
        "axes[0,3].set_ylabel('salary')\n",
        "\n",
        "axes[1,0].scatter(HR_GF_1['satisfaction_level'], HR_GF_1['average_montly_hours'], color = 'orange', marker ='s')\n",
        "axes[1,0].set_xlabel('satisfaction_level')\n",
        "axes[1,0].set_ylabel('average_montly_hours')\n",
        "\n",
        "axes[1,1].scatter(HR_GF_1['satisfaction_level'], HR_GF_1['Work_accident'], color = 'orange', marker ='s')\n",
        "axes[1,1].set_xlabel('satisfaction_level')\n",
        "axes[1,1].set_ylabel('Work_accident')\n",
        "\n",
        "axes[1,2].scatter(HR_GF_1['satisfaction_level'], HR_GF_1['promotion_last_5years'], color = 'orange', marker ='s')\n",
        "axes[1,2].set_xlabel('satisfaction_level')\n",
        "axes[1,2].set_ylabel('promotion_last_5years')\n",
        "\n",
        "axes[1,3].scatter(HR_GF_1['satisfaction_level'], HR_GF_1['salary'], color = 'orange', marker ='s')\n",
        "axes[1,3].set_xlabel('satisfaction_level')\n",
        "axes[1,3].set_ylabel('salary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ho-hLuj1bTz",
        "colab_type": "text"
      },
      "source": [
        "By comparing the top and bottom figures, the dataset is separable with respect to the left feature.\n",
        "\n",
        "**For the decision tree algorithm**, there is no need to apply one hot coding for categories features. However, we will need to convert them to numbers. We will use label encoder from sklearn library to encode the category feature (salary) as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4vXoCKhHwx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_salary = LabelEncoder()\n",
        "HR_GF_LE = pd.DataFrame.copy(HR_GF)\n",
        "HR_GF_LE['salary'] = le_salary.fit_transform(HR_GF_LE['salary'])\n",
        "HR_GF_LE.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYVPPi0N4AR_",
        "colab_type": "text"
      },
      "source": [
        "Let us define input (x) and output (y) of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ercG4Iwd4A-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = HR_GF_LE.drop('left',axis=1)\n",
        "y = HR_GF_LE.left"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfEJ_b-i3Vee",
        "colab_type": "text"
      },
      "source": [
        "Before classification, we need to split the datset into test and training parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTFsz9D_3oF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "print('size of test dataset = {}, size of traing data = {}, percentage = {}%'.format(len(x_test),len(x_train),len(x_test)*100/(len(x_test) + len(x_train))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRyJhrij4oAB",
        "colab_type": "text"
      },
      "source": [
        "Now, we are ready to apply the **decision tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wVAxuJm4pMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "HRTree = tree.DecisionTreeClassifier()\n",
        "HRTree.fit(x_train,y_train)\n",
        "ACC_train_rf = HRTree.score(x_train,y_train)\n",
        "ACC_test_rf = HRTree.score(x_test,y_test)\n",
        "print(ACC_train_rf)\n",
        "print(ACC_test_rf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMLQxxK36HRx",
        "colab_type": "text"
      },
      "source": [
        "Let us try to compare DT with logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0x7nXZuIFFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## add column for logitic regression (training)\n",
        "dm = pd.get_dummies(x_train.salary)\n",
        "x_train_lr = pd.concat([x_train,dm],axis=1)\n",
        "x_train_lr = x_train_lr.drop(['salary',2],axis=1)\n",
        "## add column for logitic regression (testing)\n",
        "dm = pd.get_dummies(x_test.salary)\n",
        "x_test_lr = pd.concat([x_test,dm],axis=1)\n",
        "x_test_lr = x_test_lr.drop(['salary',2],axis=1)\n",
        "\n",
        "\n",
        "## logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(x_train_lr, y_train)\n",
        "ACC_train_lr = model_lr.score(x_train_lr, y_train)\n",
        "ACC_test_lr = model_lr.score(x_test_lr, y_test)\n",
        "\n",
        "## Decision Trees\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "model_dt = DecisionTreeClassifier()\n",
        "model_dt.fit(x_train, y_train)\n",
        "ACC_train_dt = model_dt.score(x_train, y_train)\n",
        "ACC_test_dt = model_dt.score(x_test, y_test)\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['Accuracy', 'Logistic (%)' , 'DT (%)'])\n",
        "t.add_row(['Training', ACC_train_lr*100, ACC_train_dt*100])\n",
        "t.add_row(['Testing', ACC_test_lr*100, ACC_test_dt*100])\n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2_aAwqHLmb-",
        "colab_type": "text"
      },
      "source": [
        "**Comment on the traning and testing accuracies in the table above?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIgCwet8SjNR",
        "colab_type": "text"
      },
      "source": [
        "# Case #3: Recognition of Handwritten Digits\n",
        "\n",
        "In this section, we will try to recognize handwritten digits using **decision tree**. We will be using a standard dataset available through the sklearn library called \"load_digits\".$^{[1][2]}$\n",
        "\n",
        "[1] https://scikit-learn.org/stable/tutorial/basic/tutorial.html#introduction\n",
        "\n",
        "[2] https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n",
        "\n",
        "\n",
        "In the beginning, we will load the dataset as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thCgqt3KSpwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGRXkAwRMDWg",
        "colab_type": "text"
      },
      "source": [
        "A dataset is a dictionary-like object that holds all the data and some metadata about the data. Let us explore the content of the digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qlgSoRTAhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir(digits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEbzkkNqMLZf",
        "colab_type": "text"
      },
      "source": [
        "The digits.data contains the features that will be used to classify the digits samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lw41j9WUlr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(digits.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_ZbG54iNTgV",
        "colab_type": "text"
      },
      "source": [
        "The digits.images contains the images of the digits samples. They can be viewed using the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsM4ZMMWXGBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.gray()\n",
        "plt.matshow(digits.images[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK1cRXeTNmwf",
        "colab_type": "text"
      },
      "source": [
        "The ground truth of the datset is stored in the digits.taget"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX5YG-S-UxiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(digits.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qieqs9SM8uc1",
        "colab_type": "text"
      },
      "source": [
        "Let us use Principle Component Analysis to view the digits dataset. We will lot a projection on the 2 first principal axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pri3E2K88u8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "proj = pca.fit_transform(digits.data)\n",
        "plt.scatter(proj[:, 0], proj[:, 1], c=digits.target, cmap=\"Paired\")\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc0cKNwJN45-",
        "colab_type": "text"
      },
      "source": [
        "After exploring the content of the digits dataset, we will design a classified using **decision trees**. First, we decide the input feature vector (x) and the ground truth (y) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jSk9q-BU-1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = digits.data\n",
        "y = digits.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oti7Y9GiOg8M",
        "colab_type": "text"
      },
      "source": [
        "Then we split the datset into testing and training parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBr_laJGVLRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "print('size of test dataset = {}, size of traing data = {}, percentage = {}%'.format(len(x_test),len(x_train),len(x_test)*100/(len(x_test) + len(x_train))))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9KDqCdXOn3M",
        "colab_type": "text"
      },
      "source": [
        "Here we will train the **decision tree** model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7QAzJ0pV6Am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "model_dt = tree.DecisionTreeClassifier()\n",
        "model_dt.fit(x_train,y_train)\n",
        "ACC_train_dt = model_dt.score(x_train, y_train)\n",
        "ACC_test_dt = model_dt.score(x_test, y_test)\n",
        "print(ACC_train_dt)\n",
        "print(ACC_test_dt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig8JsexROuls",
        "colab_type": "text"
      },
      "source": [
        "Let us try to compare **DT** with the other ML techqniues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZHVdNMAMFkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(x_train, y_train)\n",
        "ACC_train_lr = model_lr.score(x_train, y_train)\n",
        "ACC_test_lr = model_lr.score(x_test, y_test)\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['Accuracy', 'Logistic (%)' , 'DT (%)'])\n",
        "t.add_row(['Training', ACC_train_lr*100, ACC_train_dt*100])\n",
        "t.add_row(['Testing', ACC_test_lr*100, ACC_test_dt*100])\n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRc0IhR1MXPu",
        "colab_type": "text"
      },
      "source": [
        "**Comment on the training and testing accuracies.**\n",
        "\n",
        "To predict the types of test samples and store it is y_pred run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uyTP8bKWvFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model_dt.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnRMgtUVO-a1",
        "colab_type": "text"
      },
      "source": [
        "Sometimes, we wish to know where did the model fail. This can be achieved using what is called the confusion matrix (discussed in more details in logistic regression)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjvAC3-ZNPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "import seaborn as sn\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(cm,annot=True)\n",
        "plt.xlabel('ground truth')\n",
        "plt.ylabel('predicted')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-IxPpjT9sdh",
        "colab_type": "text"
      },
      "source": [
        "Let us try to find out how did the **decision tree** classified a specific digit. In the next code, we will visualize the images, predicted and target values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Txo4Gmd9r1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "fig = plt.figure(figsize=(12, 24))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "digit_visualize = 5 # the digit that we want to visualize\n",
        "\n",
        "cnt = 0\n",
        "i = 0\n",
        "while (i < 128) and (i < len(y_test)) :\n",
        "    if y_test[i] == digit_visualize:\n",
        "       Idx = np.where(np.prod(digits.data == x_test[i,:],axis = -1))\n",
        "       ax = fig.add_subplot(16, 8, cnt + 1, xticks=[], yticks=[])\n",
        "       ax.imshow(digits.images[int(Idx[0])], cmap=plt.cm.binary, interpolation='nearest')\n",
        "       # label the image with the target value\n",
        "       ax.text(0, 7, str(y_test[i]))\n",
        "       ax.text(6.5, 7, str(y_pred[i]))\n",
        "       cnt+=1\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFCzYJXo9raV",
        "colab_type": "text"
      },
      "source": [
        "Let us try to find out in more details where did the **decision tree** failed. In the next code, we will visualize the images, predicted and target values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cIMeLwx9tf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(12, 24))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "cnt = 0\n",
        "i = 0\n",
        "while (i < 128) and (i < len(y_test)) :\n",
        "    if y_test[i] != y_pred[i]:\n",
        "       Idx = np.where(np.prod(digits.data == x_test[i,:],axis = -1))\n",
        "       ax = fig.add_subplot(16, 8, cnt + 1, xticks=[], yticks=[])\n",
        "       ax.imshow(digits.images[int(Idx[0])], cmap=plt.cm.binary, interpolation='nearest')\n",
        "       # label the image with the target value\n",
        "       ax.text(0, 7, str(y_test[i]))\n",
        "       ax.text(6.5, 7, str(y_pred[i]))\n",
        "       cnt+=1\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clLdKNnsVLSK",
        "colab_type": "text"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "**1) Iris identification**: use decision tree classification for the iris dataset stored in the sklearn library. You may import the dataset using \"from sklearn.datasets import load_iris\"."
      ]
    }
  ]
}